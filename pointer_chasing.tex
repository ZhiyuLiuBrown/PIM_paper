
%\section{Pointer-Chasing Data Structures}
\section{Low Contention Data Structures}
\label{section:pointer_chasing}

In this section we consider data structures with low contention; pointer chasing data structures, 
such as linked-lists and skip-lists, fall in this category.  
These are data structures whose operations  
need to de-reference a non-constant sequence of pointers before completing. We assume they support operations 
such as add($x$), delete($x$) and contains($x$), which follow ``next node" pointers until 
reaching the position of node $x$.
When these data structures access uniformly random keys and when they are too large to fit in CPU caches, 
they incur expensive memory accesses, which cannot
be easily predicted, making the pointer chasing the dominating overhead of these data structures.
Naturally, these data structures have been early examples of the benefit of near-memory 
computing~\cite{}, as the entire pointer chase could be performed by the PIM core, and only the final result returned to 
the application. 

However, under the same conditions, these data structures have inherently low contetion. Lock-free algorithms~\cite{}
have shown that these data structures can scale to hundreds of cores under low contention. Unfortunately, each vault in 
PIM memory has a single core; as a consequence, prior work 
has only compared PIM data structures with sequential data structures, 
not with carefully crafted concurrent data structures.

We analyze linked-lists and skip-lists, and show that the naive PIM data structure in each case cannot 
outperform the equivalent CPU managed concurrent data structure even for a small number of cores. Next, 
we show how to use state-of-the art techniques from concurrent computing literature to 
optimize algorithms for near-memory computing to outperform well-known concurrent data 
structures. 

%\subsection{PIM-managed linked-list}
\subsection{Linked-lists}
\label{section:linked_list}
We now describe a naive PIM linked-list.
The linked-list is stored in a vault, maintained by the local PIM core.
Whenever a CPU\footnote{We use the term CPU to refer to CPU cores, as oposed to PIM cores.} 
wants to perform an operation on the linked-list,
it sends a request to the PIM core.
The PIM core will retrieve the message, execute the operation, and send the result back to the CPU.
The PIM linked-list is sequential, as it can only be accessed by one PIM core. 

Doing pointer chasing on sequential data structures by PIM cores is not a new idea
(e.g., \cite{hsieh2016accelerating, Ahn2015:2}).
It is obvious that for a sequential data structure like a sequential linked-list,
replacing the CPU with a PIM core to access the data structure will largely improve
its performance due to the PIM core's much faster memory access.
However, we are not aware of any prior comparison between the performance of
PIM-managed data structures and concurrent data structures
in which CPUs can make operations in parallel.
In fact, our analytical and experimental results will show that
the performance of the naive PIM-managed linked-list is much worse than
that of the concurrent linked-list with fine-grained locks \cite{Heller05}.

To improve the performance of the PIM-managed linked-list,
we apply the following \textit{combining optimization} to it:
the PIM core retrieves all pending requests from its buffer and
executes all of them during only one traversal over the linked-list.
It is not hard to see that the role of the PIM core in our PIM-managed linked-list
is very similar to that of the combiner in a concurrent linked-list implemented
using \textit{flat combining} \cite{Hendler10}, where, roughly speaking,
threads compete for a ``combiner lock" to become the combiner, and
the combiner will take over all operation requests from other threads and execute them.
Therefore, we think the performance of the flat-combining linked-list is a good indicator to
the performance of our PIM-managed linked-list.

Based on our performance model, we can calculate the approximate expected
throughputs of the linked-list algorithms mentioned above, 
when there are $p$ CPUs making operation requests concurrently.
We assume that a linked-list consists of nodes with integer keys in the range of $[1, N]$.
Initially a linked-list has $n$ nodes with keys generated independently
and uniformly at random from $[1, N]$.
The keys of the operation requests are generated the same way.
To simplify the analysis, we assume that CPUs only make $contains()$ requests
(or the number of $add()$ requests is the same as the number of $delete()$
so that the size of each linked-list nearly doesn't change).
We also assume that a CPU makes a new operation request immediately after
its previous one completes.
Assuming that $n \gg p$ and $N \gg p$, the approximate expected throughputs (per second) 
of the concurrent linked-lists are presented in Table \ref{tab:linkedlist}, 
where $\Sp = \sum\limits_{i=1}^{n} ({i \over n+1})^{p}$.\footnote{
We define the rank of an operation request to a linked-list as the number of pointers
it has to traverse until it finds the right position for it in the linked-list.
$\Sp$ is the expected rank of the operation request with the biggest key
among $p$ random requests a PIM core or a combiner has to combine,
which is essentially the expected number of pointers a PIM core or a combiner
has to go through during one pointer chasing procedure.}

\begin{table}[ht!]
\begin{center}
    \begin{tabular}{| >{\small}l | l |}
    \hline
    Algorithm & Throughput\\ \hline
    Linked-list with fine-grained locks & ${2p \over (n+1)\latcpu}$ \\ \hline
    Flat-combining linked-list without combining & ${2 \over (n+1)\latcpu}$ \\ \hline
    PIM-managed linked-list without combining & ${2 \over (n+1)\latpim}$ \\ \hline
    Flat-combining linked-list with combining & ${p \over (n-\Sp)\latcpu}$ \\ \hline
    PIM-managed linked-list with combining & ${p \over (n-\Sp)\latpim}$ \\ \hline
    \end{tabular}
\end{center}
\caption{Throughputs of linked-list algorithms.}
\label{tab:linkedlist}
\end{table}


It is easy to see that the PIM-managed linked-list with combining outperforms 
the linked-list with fine-grained locks, which is the best one among other algorithms, 
as long as ${\latcpu \over \latpim} > {2(n - \Sp)\over n + 1}$.
Given that $0 < \Sp \le {n \over 2}$ and $\latcpu = 3\latpim$,
the throughput of the PIM-managed linked-list with combining should be at least 
1.5 times the throughput of the linked-list with fine-grained locks.
Without combining, however, the PIM-managed linked-list cannot
beat the linked-list with fine-grained locks when $p > 6$.

We implemented the linked-list with fine-grained locks and the flat-combining link-list 
with and without the combining optimization.
We tested them on a Dell server with 512 GB RAM and 
56 cores on four Intel Xeon E7-4850v3 processors at 2.2 GHz.
To get rid of NUMA access effects, we ran experiments with only one processor, 
which is a NUMA node with 14 cores, a 35 MB shared L3 cache, 
and a private L2/L1 cache of size 256 KB/64 KB per core. 
Each core has 2 hyperthreads, for a total of 28 hyperthreads. 
Cache lines have 64 bytes.

The throughputs of the algorithms are presented in Figure \ref{figure:linkedlist_data}.
The results confirmed the validity of our analysis in Table \ref{tab:linkedlist}.
The throughput of the flat-combining algorithm without combining optimization
is much worse than the algorithm with fine-grained locks.
Since we believe the performance of the flat-combining linked-list is a good 
indicator of that of the PIM-managed linked-list, we triple the throughput of the
flat-combining algorithm without combining optimization to get the estimated
throughput of the PIM-managed algorithm. 
As we can see, it is still far below the throughput of the one with fined-grained locks.
However, with the combining optimization, the performance of the flat-combining
algorithm improves significantly and the estimated throughput of our PIM-managed
linked-list with combining optimization now beats all others'.

\begin{comment}
\begin{figure}[ht!]
    \centering
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=.9\linewidth]{linkedlist_data.eps} % first figure itself
        \caption{Experimental results of linked-lists}
        \label{figure:linkedlist_data}
    \end{minipage}\hfill
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=.98\linewidth]{skiplist_data.eps} % second figure itself
        \caption{Experimental results of skip-lists}
        \label{figure:skiplist_data}
    \end{minipage}
\end{figure}
\end{comment}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{linkedlist_data.eps} % first figure itself
    \caption{Experimental results of linked-lists. We evaluated the linked-list with Fine-grained locks 
        and the flat-combining linked-list (FC) with and without the combining optimization.}
   \label{figure:linkedlist_data}
\end{figure}


\subsection{Skip-lists}
\label{section:skip_list}

%\subsubsection{Base algorithm}

Like the naive PIM-managed linked-list,
the naive PIM-managed skip-list keeps the skip-list in a single vault and
CPUs send operation requests to the local PIM core that executes those operations.
As we will see, this algorithm is less efficient than some existing algorithms.

Unfortunately, the combining optimization cannot be applied to skip-lists effectively.
The reason is that for any two nodes not close enough to each other in the skip-list,
the paths we traverse through to reach them don't largely overlap.

On the other hand, PIM memory usually consists of many vaults and PIM cores.
For instance, the first generation of Hybrid Memory Cube \cite{website:HMC} has up to 32 vaults.
Hence, a PIM-managed skip-list may achieve much better performance if
we can exploit the parallelism of multiple vaults.
Here we present our PIM-managed skip-list with a \textit{partitioning optimization}:
A skip-list is divided into partitions of disjoint ranges of keys,
stored in different vaults, so that a CPU sends its operation request to
the PIM core of the vault to which the key of the operation belongs.

Figure \ref{figure:skiplist_structure} illustrates the structure of a PIM-managed skip-list.
Each partition of a skip-list starts with a \textit{sentinel node}
which is a node of the max height. 
For simplicity, assume the max height $H_{max}$ is predefined.
A partition covers a key range between the key of its sentinel node and
the key of the sentinel node of the next partition.
CPUs also store a copy of each sentinel node in the normal DRAM and 
the copy has an extra variable indicating the vault containing the sentinel node.
Since the number of nodes of the max height is very small with high probability, 
those copies of those sentinel nodes can almost certainly stay in cache
if CPUs access them frequently.

When a CPU applies an operation for a key to the skip-list,
it first compares the key with those of the sentinels, discovers which vault
the key belongs to, and then sends its operation request to that vault's PIM core.
Once the PIM core retrieves the request, it executes the operation in the local vault 
and finally sends the result back to the CPU.


\begin{figure}[ht!]
%$\hrulefill$
%\\
%\\
\centering
\includegraphics[width=1.0\linewidth]{skiplist_structure.eps}
%$\hrulefill$
\caption{A PIM-managed FIFO queue with three partitions}
\label{figure:skiplist_structure}
\end{figure}

Now let us discuss how we implement the PIM-managed skip-list
when the key of each operation is an integer generated uniformly at random
from range $[0, n]$ and the PIM memory has $k$ vaults available.
Initially we can create $k$ partitions starting with fake sentinel nodes
with keys $0$, $1/k$, $2/k$,..., $(n-1)/k$, respectively, 
and allocate each partition in a different vault. 
The sentinel nodes will never be deleted.
If a new node to be added has the same key as a sentinel node,
we insert it immediately after the sentinel node.

We compare the performance of our PIM-managed skip-list with partitions 
to the performance of a flat-combining skip-list \cite{Hendler10}
and a lock-free skip-list \cite{Herlihy08}, 
where $p$ CPUs keeps making operation requests.
We also apply the partitioning optimization to the flat-combining skip-list, 
so that $k$ combiners are in charge of $k$ partitions of the skip-list. 
To simplify the comparison, we assume that all skip-lists have the same
initial structure (expect that skip-lists with partitions have extra sentinel nodes)
and all the operations are contains() operations
(or the number of $add()$ requests is the same as the number of $delete()$ 
so that the size of each skip-list nearly doesn't change).
Their approximate expected throughputs are presented in Table \ref{tab:skiplist}, 
where $\beta$ is the average number of nodes an operation has to go through
in order to find the location of its key in a skip-list
($\beta = \Theta(\log N)$, where $N$ is the size of the skip-list).
Note that we have ignored some overheads in the flat-combining
algorithms, such as maintaining combiner locks and publication lists
(we will discuss publication lists in more detail in Section \ref{section:contended}).
We also have overestimated the performance of the lock-free skip-list by not counting the
CAS operations used in add() and delete() requests, as well as the cost of retries
caused by conflicts of updates.
Even so, our PIM-managed linked-list with partitioning optimization is
still expected to outperform the second best algorithm, the lock-free skip-list 
when $k > {(\beta\latpim + \latmes)p \over \beta\latcpu}$.
Given that $\latmes = \latcpu = 3\latpim$, $k > p/3$ should suffice.

\begin{table}[ht!]
\begin{center}
	\begin{tabular}{| >{\small}l | l |}
    %\begin{tabular}{| m{0.68\linewidth}  | l |}
    \hline
    Algorithm & Throughput\\ \hline
    Look-free skip-list & ${p \over \beta\latcpu}$ \\ \hline
    Flat-combining skip-list & ${1 \over \beta\latcpu}$ \\ \hline
    PIM-managed skip-list & ${1 \over (\beta\latpim + \latmes)}$ \\ \hline
    Flat-combining skip-list with $k$ partitions & ${k \over \beta\latcpu}$ \\ \hline
    PIM-managed skip-list with $k$ partitions & ${k \over (\beta\latpim + \latmes)}$ \\ \hline
    \end{tabular}
\end{center}
\caption{Throughputs of skip-list algorithms.}
\label{tab:skiplist}
\end{table}

Our experiments have revealed similar results, 
as presented in Figure \ref{figure:skiplist_data}.
We have implemented and run the flat-combining skip-list with different numbers of
partitions and compared them with the lock-free skip-list.
As the number of partitions increases, the performance of the flat-combining skip-list
gets better, implying the effectiveness of the partitioning optimization.
Again we believe the performance of the flat-combining skip-list is a good indicator
to the performance of our PIM-managed skip-list.
Therefore, according to the analytical results in Table \ref{tab:skiplist}, we can triple the throughput 
of a flat-combining skip-list to get the expected performance of a PIM-managed skip-list.
As Figure \ref{figure:skiplist_data} illustrates, when the PIM-managed skip-list has $8$ or $16$ 
partitions, it is expected to outperform the lock-free skip-list with up to 28 hardware threads.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{skiplist_data.eps} % second figure itself
    \caption{Experimental results of skip-lists. We evaluated the lock-free skip-list and 
    the flat-combining skip-list (FC) with different numbers (1, 4, 8, 16) of partitions.}
    \label{figure:skiplist_data}
\end{figure}


%\subsubsection{Rebalancing skip-list}
\subsubsection{Skip-list Rebalancing}
The PIM-managed skip-list performs well with a uniform distribution of requests.
However, if the distribution of requests is not uniform, a static partitioning scheme 
will result in unbalanced partitions, with some PIM cores being idle, while others having to 
serve a majority of requests. To address this problem, we introduce a non-blocking protocol for 
migrating consecutive nodes from one vault to another. 

The protocol works as follows. 
A PIM core $p$ that manages a vault $v'$ can send a message to another PIM core $q$, managing vault 
$v$, to request that some nodes are moved from $v'$ to $v$. 
First, $p$ sends a message notifying $q$ of the start of the migration. 
Then $p$ sends messages of adding those nodes to $q$ one by one in an ascending order 
according to the keys of the nodes. 
After all the nodes have been migrated, $p$ sends notification messages to CPUs so that 
they can update their copies of sentinel nodes accordingly.
After $p$ receives acknowledgement messages from all CPUs, it notifies $q$ of the end of migration.
To keep the node migration protocol simple, we don't allow $q$ to move those nodes 
to another vault again until $p$ finishes its node migration. 

During the node migration, $p$ can still serve requests from CPUs.
Assume that a request with key $k_1$ is sent to $p$ when $p$ is migrating nodes 
in a key range containing $k_1$.  
If $p$ is about to migrate a node with key $k_2$ at the moment and $k_1 \ge k_2$, 
$p$ serves the request itself. 
Otherwise, $p$ must have migrated all nodes in the subset containing key $k_1$, and therefore $p$ 
forwards the request to $q$ which will serve the request and respond directly to the requesting CPU. 

The algorithm is correct, because a request will eventually reach the vault that 
currently contains nodes in the key range that the request belongs to: 
If a request arrives to $p$ which no longer holds the partition the request belongs to, 
$p$ can simply reply with a rejection to the CPU and the CPU will resend its request to 
the correct PIM core, 
because it has already updated its sentinels and knows which PIM core it should contact now. 

Using this node migration protocol, the PIM-managed FIFO queue can support two rebalancing schemes:
1) If a partition has too many nodes, the local PIM core can send nodes in a key range to a vault 
that has fewer nodes;
2) If two consecutive partitions are both small, 
we can merge then by moving one to the vault containing the other. 

In practice, we expect that rebalancing will not happen very frequently, so its overhead can be 
ameliorated by the improved efficiency resulting from a rebalance. 

