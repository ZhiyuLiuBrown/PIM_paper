%\section{Processing in Memory}
%\label{section:model}
\section{Hardware Architecture and Model}


%\subsection{Hardware model}
\label{section:hardware_model}

\begin{figure}[ht!]
%$\hrulefill$
%\\
%\\
\centering
\includegraphics[width=.9\linewidth]{model.eps}
%$\hrulefill$
\caption{The PIM model}
\label{figure:model}
\end{figure}

In the PIM hardware model, multiple CPUs are connected to the main
memory, via a shared crossbar network, as illustrated in Figure \ref{figure:model}.
The main memory consists of two parts---one is a normal DRAM accessible by CPUs 
and the other, called the \textit{PIM memory}, is divided into multiple partitions, 
called \textit{PIM vaults} or simply vaults.  
According to the Hybrid Memory Cube specification 1.0 \cite{website:HMC}, each HMC consists of 16 or 
32 vaults and has total size 2GB or 4 GB (so each vault has size roughly 100MB). 
We assume the same specifications in our PIM model, although the size of a PIM memory and 
the number of its vaults can be greater. 
Each CPU also has access to a hierarchy of caches backed by DRAM,
and there can be last-level caches shared among multiple CPUs. 

Each vault has a \textit{PIM core} directly attached to it.
we say a vault is \textit{local} to the PIM core attached to it, and vice versa.
A PIM core is a lightweight CPU that may be slower than a full-fledged CPU
with respect to computation speed.\footnote{
A PIM core can be thought of as an in-order CPU with only small private L1 cache and 
without some optimizations that full-fledged CPUs usually have.}
A vault can be accessed only by its local PIM core.\footnote{
We may alternatively assume that a PIM core has direct access to remote vaults, at a larger cost. 
We may also assume that vaults are accessible by CPUs as well, 
but at the cost of dealing with cache coherence between CPUs and PIM cores. 
Some cache coherence mechanisms for PIM memory have be proposed 
and claimed to be not costly (e.g., \cite{boroumand2016}). 
However, we prefer to keep the hardware model simple and we will show that we are still able to 
design efficient concurrent data structure algorithms with this simple, less powerful PIM memory.}
Although a PIM core is relatively slow computationally, it has fast access to its local vault.

A PIM core communicates with other PIM cores and CPUs via messages.
Each PIM core, as well as each CPU, has buffers for storing incoming messages.
A message is guaranteed to eventually arrive at the buffer of its receiver.
Messages from the same sender to the same receiver are delivered in FIFO order: 
the message sent first arrives at the receiver first. 
However, messages from different senders or to different receivers can arrive in an arbitrary order. 

To keep the PIM memory simple, we assume that a PIM core can only make read and write operations 
to its local vault, while a CPU also supports more powerful atomic operations, such as CAS and F\&A.
Virtual memory is cheap to be achieved in this model, 
by having each PIM core maintain its own page table for its local vault~\cite{}.

%TODO
[DISCUSS CACHE COHERENCE]