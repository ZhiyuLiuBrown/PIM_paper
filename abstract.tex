\begin{abstract}

The performance gap between memory and CPU has grown exponentially. 
To bridge this gap, hardware architects have proposed 
near-memory computing (also called processing-in-memory, or PIM), 
where a lightweight processor (called a PIM core) is located close to 
memory. 
Due to its proximity, a memory access from a PIM core is much faster than from a CPU core.
New advances in 3D 
integration and in die stacked memory make PIM viable in the 
near future. Prior work has shown significant performance improvements by using PIM for 
embarrassingly parallel and data-intensive applications, as well as for
pointer-chasing traversals in \emph{sequential} data structures. 
However, current server machines have hundreds of cores; algorithms for 
concurrent data structures exploit these cores to achieve high throughput and 
scalability, with significant benefits over sequential data structures. 

In this paper, we show that
  naive PIM data structures cannot outperform 
 state-of-the-art \emph{concurrent} data structures. 
We 
investigate two classes of data structures. First, we analyze pointer chasing data 
structures, which have a high degree of inherent parallelism and low contention, but incur significant 
overhead due to unpredictable memory accesses. Second, we explore
FIFO queues, which can leverage CPU caches to exploit their inherent high locality, but have a high degree of contention. 
We propose new designs for PIM 
data structures using 
techniques such as combining, partitioning and pipelining, that can outperform traditional 
concurrent data structures, with a significantly simpler design. 


\end{abstract}
